{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A suite of utilities for AWS Lambda functions to ease adopting best practices such as tracing, structured logging, custom metrics, and more. Looking for a quick read through how the core features are used? Check out this detailed blog post with a practical example. Tenets \u00b6 This project separates core utilities that will be available in other runtimes vs general utilities that might not be available across all runtimes. AWS Lambda only . We optimise for AWS Lambda function environments and supported runtimes only. Utilities might work with web frameworks and non-Lambda environments, though they are not officially supported. Eases the adoption of best practices . The main priority of the utilities is to facilitate best practices adoption, as defined in the AWS Well-Architected Serverless Lens; all other functionality is optional. Keep it lean . Additional dependencies are carefully considered for security and ease of maintenance, and prevent negatively impacting startup time. We strive for backwards compatibility . New features and changes should keep backwards compatibility. If a breaking change cannot be avoided, the deprecation and migration process should be clearly defined. We work backwards from the community . We aim to strike a balance of what would work best for 80% of customers. Emerging practices are considered and discussed via Requests for Comment (RFCs) Idiomatic . Utilities follow programming language idioms and language-specific best practices. Install \u00b6 Powertools is available on NuGet. AWS.Lambda.PowerTools : dotnet nuget add AWS.Lambda.PowerTools Quick hello world example using SAM CLI 1 sam init --location https://github.com/aws-samples/cookiecutter-aws-sam-dotnet Features \u00b6 Utility Description Tracing Decorators and utilities to trace Lambda function handlers, and both synchronous and asynchronous functions Logger Structured logging made easier, and decorator to enrich structured logging with key Lambda context details Metrics Custom Metrics created asynchronously via CloudWatch Embedded Metric Format (EMF) Environment variables \u00b6 Info Explicit parameters take precedence over environment variables. Environment variable Description Utility Default POWERTOOLS_SERVICE_NAME Sets service name used for tracing namespace, metrics dimension and structured logging All \"service_undefined\" POWERTOOLS_METRICS_NAMESPACE Sets namespace used for metrics Metrics None POWERTOOLS_TRACE_DISABLED Disables tracing Tracing false POWERTOOLS_TRACER_CAPTURE_RESPONSE Captures Lambda or method return as metadata. Tracing true POWERTOOLS_TRACER_CAPTURE_ERROR Captures Lambda or method exception as metadata. Tracing true POWERTOOLS_LOGGER_LOG_EVENT Logs incoming event Logging false POWERTOOLS_LOGGER_SAMPLE_RATE Debug log sampling Logging 0 POWERTOOLS_LOG_DEDUPLICATION_DISABLED Disables log deduplication filter protection to use Pytest Live Log feature Logging false","title":"Homepage"},{"location":"#tenets","text":"This project separates core utilities that will be available in other runtimes vs general utilities that might not be available across all runtimes. AWS Lambda only . We optimise for AWS Lambda function environments and supported runtimes only. Utilities might work with web frameworks and non-Lambda environments, though they are not officially supported. Eases the adoption of best practices . The main priority of the utilities is to facilitate best practices adoption, as defined in the AWS Well-Architected Serverless Lens; all other functionality is optional. Keep it lean . Additional dependencies are carefully considered for security and ease of maintenance, and prevent negatively impacting startup time. We strive for backwards compatibility . New features and changes should keep backwards compatibility. If a breaking change cannot be avoided, the deprecation and migration process should be clearly defined. We work backwards from the community . We aim to strike a balance of what would work best for 80% of customers. Emerging practices are considered and discussed via Requests for Comment (RFCs) Idiomatic . Utilities follow programming language idioms and language-specific best practices.","title":"Tenets"},{"location":"#install","text":"Powertools is available on NuGet. AWS.Lambda.PowerTools : dotnet nuget add AWS.Lambda.PowerTools Quick hello world example using SAM CLI 1 sam init --location https://github.com/aws-samples/cookiecutter-aws-sam-dotnet","title":"Install"},{"location":"#features","text":"Utility Description Tracing Decorators and utilities to trace Lambda function handlers, and both synchronous and asynchronous functions Logger Structured logging made easier, and decorator to enrich structured logging with key Lambda context details Metrics Custom Metrics created asynchronously via CloudWatch Embedded Metric Format (EMF)","title":"Features"},{"location":"#environment-variables","text":"Info Explicit parameters take precedence over environment variables. Environment variable Description Utility Default POWERTOOLS_SERVICE_NAME Sets service name used for tracing namespace, metrics dimension and structured logging All \"service_undefined\" POWERTOOLS_METRICS_NAMESPACE Sets namespace used for metrics Metrics None POWERTOOLS_TRACE_DISABLED Disables tracing Tracing false POWERTOOLS_TRACER_CAPTURE_RESPONSE Captures Lambda or method return as metadata. Tracing true POWERTOOLS_TRACER_CAPTURE_ERROR Captures Lambda or method exception as metadata. Tracing true POWERTOOLS_LOGGER_LOG_EVENT Logs incoming event Logging false POWERTOOLS_LOGGER_SAMPLE_RATE Debug log sampling Logging 0 POWERTOOLS_LOG_DEDUPLICATION_DISABLED Disables log deduplication filter protection to use Pytest Live Log feature Logging false","title":"Environment variables"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. This project follows Keep a Changelog format for changes and adheres to Semantic Versioning . [Unreleased] \u00b6 [0.1.0] - 2021-11-15 \u00b6 Added \u00b6 Public beta release","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. This project follows Keep a Changelog format for changes and adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#010-2021-11-15","text":"","title":"[0.1.0] - 2021-11-15"},{"location":"changelog/#added","text":"Public beta release","title":"Added"},{"location":"core/logger/","text":"Logger provides an opinionated logger with output structured as JSON. Key features \u00b6 Capture key fields from Lambda context, cold start and structures logging output as JSON Log Lambda event when instructed (disabled by default) Log sampling enables DEBUG log level for a percentage of requests (disabled by default) Append additional keys to structured log at any point in time Getting started \u00b6 Logger requires two settings: Setting Description Environment variable Constructor parameter Logging level Sets how verbose Logger should be (INFO, by default) LOG_LEVEL level Service Sets service key that will be present across all log statements POWERTOOLS_SERVICE_NAME service Example using AWS Serverless Application Model (SAM) template.yaml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : example Function.cs 1 2 3 using AWS.Lambda.PowerTools.Logging // TODO: finish the initialisation sample Standard structured keys \u00b6 Your Logger will include the following keys to your structured logging: Key Example Note level : str INFO Logging level location : str collect.handler:1 Source code location where statement was executed message : Any Collecting payment Unserializable JSON values are casted as str timestamp : str 2021-05-03 10:20:19,650+0200 Timestamp with milliseconds, by default uses local timezone service : str payment Service name defined, by default service_undefined xray_trace_id : str 1-5759e988-bd862e3fe1be46a994272793 When tracing is enabled , it shows X-Ray Trace ID sampling_rate : float 0.1 When enabled, it shows sampling rate in percentage e.g. 10% exception_name : str ValueError When logger.exception is used and there is an exception exception : str Traceback (most recent call last).. When logger.exception is used and there is an exception Capturing Lambda context info \u00b6 You can enrich your structured logs with key Lambda context information via inject_lambda_context . Function.cs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 using AWS.Lambda.PowerTools.Logging logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) # You can log entire objects too logger . info ({ \"operation\" : \"collect_payment\" , \"charge_id\" : event [ ' charge_id ' ] }) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : { \"operation\" : \"collect_payment\" , \"charge_id\" : \"ch_AZFlk2345C0\" }, \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } When used, this will include the following keys: Key Example cold_start : bool false function_name str example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_memory_size : int 128 function_arn : str arn:aws:lambda:eu-west-1:012345678910:function:example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_request_id : str 899856cb-83d1-40d7-8611-9e78f15f32f4 Logging incoming event \u00b6 When debugging in non-production environments, you can instruct Logger to log the incoming event with log_event param or via POWERTOOLS_LOGGER_LOG_EVENT env var. Warning This is disabled by default to prevent sensitive info being logged. log_handler_event.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( log_event = True ) def handler ( event , context ): ... Setting a Correlation ID \u00b6 You can set a Correlation ID using correlation_id_path param by passing a JMESPath expression . You can retrieve correlation IDs via get_correlation_id method collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = \"headers.my_request_id_header\" ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"headers\" : { \"my_request_id_header\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" } We provide built-in JMESPath expressions for known event sources, where either a request ID or X-Ray Trace ID are present. collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" } Appending additional keys \u00b6 Custom keys are persisted across warm invocations Always set additional keys as part of your handler to ensure they have the latest value, or explicitly clear them with clear_state=True . You can append additional keys using either mechanism: Persist new keys across all future log messages via append_keys method Add additional keys on a per log message basis via extra parameter append_keys method \u00b6 NOTE: append_keys replaces structure_logs(append=True, **kwargs) method. Both will continue to work until the next major version. You can append your own keys to your existing Logger via append_keys(**additional_key_values) method. collect.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): order_id = event . get ( \"order_id\" ) # this will ensure order_id key always has the latest value before logging logger . append_keys ( order_id = order_id ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:11\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"order_id\" : \"order_id_value\" } Logger will automatically reject any key with a None value If you conditionally add keys depending on the payload, you can follow the example above. This example will add order_id if its value is not empty, and in subsequent invocations where order_id might not be present it'll remove it from the Logger. extra parameter \u00b6 Extra parameter is available for all log levels' methods, as implemented in the standard logging library - e.g. logger.info, logger.warning . It accepts any dictionary, and all keyword arguments will be added as part of the root structure of the logs for that log statement. Any keyword argument added using extra will not be persisted for subsequent messages. extra_parameter.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) fields = { \"request_id\" : \"1123\" } logger . info ( \"Collecting payment\" , extra = fields ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:6\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"request_id\" : \"1123\" } set_correlation_id method \u00b6 You can set a correlation_id to your existing Logger via set_correlation_id(value) method by passing any string value. collect.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . set_correlation_id ( event [ \"requestContext\" ][ \"requestId\" ]) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"correlation_id\" : \"correlation_id_value\" } Alternatively, you can combine Data Classes utility with Logger to use dot notation object: collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent logger = Logger ( service = \"payment\" ) def handler ( event , context ): event = APIGatewayProxyEvent ( event ) logger . set_correlation_id ( event . request_context . request_id ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:9\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" } Removing additional keys \u00b6 You can remove any additional key from Logger state using remove_keys . collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( sample_key = \"value\" ) logger . info ( \"Collecting payment\" ) logger . remove_keys ([ \"sample_key\" ]) logger . info ( \"Collecting payment without sample key\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"sample_key\" : \"value\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment without sample key\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" } Clearing all state \u00b6 Logger is commonly initialized in the global scope. Due to Lambda Execution Context reuse , this means that custom keys can be persisted across invocations. If you want all custom keys to be deleted, you can use clear_state=True param in inject_lambda_context decorator. Info This is useful when you add multiple custom keys conditionally, instead of setting a default None value if not present. Any key with None value is automatically removed by Logger. This can have unintended side effects if you use Layers Lambda Layers code is imported before the Lambda handler. This means that clear_state=True will instruct Logger to remove any keys previously added before Lambda handler execution proceeds. You can either avoid running any code as part of Lambda Layers global scope, or override keys with their latest value as part of handler's execution. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( clear_state = True ) def handler ( event , context ): if event . get ( \"special_key\" ): # Should only be available in the first request log # as the second request doesn't contain `special_key` logger . append_keys ( debugging_key = \"value\" ) logger . info ( \"Collecting payment\" ) #1 request 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"special_key\" : \"debug_key\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } #2 request 1 2 3 4 5 6 7 8 9 10 11 12 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : false , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } Logging exceptions \u00b6 Use logger.exception method to log contextual information about exceptions. Logger will include exception_name and exception keys to aid troubleshooting and error enumeration. Tip You can use your preferred Log Analytics tool to enumerate and visualize exceptions across all your services using exception_name key. collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) try : raise ValueError ( \"something went wrong\" ) except Exception : logger . exception ( \"Received an exception\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"level\" : \"ERROR\" , \"location\" : \"collect.handler:5\" , \"message\" : \"Received an exception\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"exception_name\" : \"ValueError\" , \"exception\" : \"Traceback (most recent call last):\\n File \\\"<input>\\\", line 2, in <module>\\nValueError: something went wrong\" } Advanced \u00b6 Reusing Logger across your code \u00b6 Logger supports inheritance via child parameter. This allows you to create multiple Loggers across your code base, and propagate changes such as new keys to all Loggers. collect.py 1 2 3 4 5 6 7 8 import shared # Creates a child logger named \"payment.shared\" from aws_lambda_powertools import Logger logger = Logger () # POWERTOOLS_SERVICE_NAME: \"payment\" def handler ( event , context ): shared . inject_payment_id ( event ) ... shared.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( child = True ) # POWERTOOLS_SERVICE_NAME: \"payment\" def inject_payment_id ( event ): logger . structure_logs ( append = True , payment_id = event . get ( \"payment_id\" )) In this example, Logger will create a parent logger named payment and a child logger named payment.shared . Changes in either parent or child logger will be propagated bi-directionally. Child loggers will be named after the following convention {service}.{filename} If you forget to use child param but the service name is the same of the parent, we will return the existing parent Logger instead. Sampling debug logs \u00b6 Use sampling when you want to dynamically change your log level to DEBUG based on a percentage of your concurrent/cold start invocations . You can use values ranging from 0.0 to 1 (100%) when setting POWERTOOLS_LOGGER_SAMPLE_RATE env var or sample_rate parameter in Logger. When is this useful? Let's imagine a sudden spike increase in concurrency triggered a transient issue downstream. When looking into the logs you might not have enough information, and while you can adjust log levels it might not happen again. This feature takes into account transient issues where additional debugging information can be useful. Sampling decision happens at the Logger initialization. This means sampling may happen significantly more or less than depending on your traffic patterns, for example a steady low number of invocations and thus few cold starts. Note If you want Logger to calculate sampling upon every invocation, please open a feature request . collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger # Sample 10% of debug logs e.g. 0.1 logger = Logger ( service = \"payment\" , sample_rate = 0.1 ) def handler ( event , context ): logger . debug ( \"Verifying whether order_id is present\" ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Verifying whether order_id is present\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 } LambdaPowertoolsFormatter \u00b6 Logger propagates a few formatting configurations to the built-in LambdaPowertoolsFormatter logging formatter. If you prefer configuring it separately, or you'd want to bring this JSON Formatter to another application, these are the supported settings: Parameter Description Default json_serializer function to serialize obj to a JSON formatted str json.dumps json_deserializer function to deserialize str , bytes , bytearray containing a JSON document to a Python obj json.loads json_default function to coerce unserializable values, when no custom serializer/deserializer is set str datefmt string directives (strftime) to format log timestamp %Y-%m-%d %H:%M:%S,%F%z , where %F is a custom ms directive utc set logging timestamp to UTC False log_record_order set order of log keys when logging [\"level\", \"location\", \"message\", \"timestamp\"] kwargs key-value to be included in log messages None LambdaPowertoolsFormatter.py 1 2 3 4 5 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter formatter = LambdaPowertoolsFormatter ( utc = True , log_record_order = [ \"message\" ]) logger = Logger ( service = \"example\" , logger_formatter = formatter ) Migrating from other Loggers \u00b6 If you're migrating from other Loggers, there are few key points to be aware of: Service parameter , Inheriting Loggers , Overriding Log records , and Logging exceptions . The service parameter \u00b6 Service is what defines the Logger name, including what the Lambda function is responsible for, or part of (e.g payment service). For Logger, the service is the logging key customers can use to search log operations for one or more functions - For example, search for all errors, or messages like X, where service is payment . Inheriting Loggers \u00b6 Python Logging hierarchy happens via the dot notation: service , service.child , service.child_2 For inheritance, Logger uses a child=True parameter along with service being the same value across Loggers. For child Loggers, we introspect the name of your module where Logger(child=True, service=\"name\") is called, and we name your Logger as {service}.{filename} . Danger A common issue when migrating from other Loggers is that service might be defined in the parent Logger (no child param), and not defined in the child Logger: incorrect_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( child = True ) correct_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" , child = True ) In this case, Logger will register a Logger named payment , and a Logger named service_undefined . The latter isn't inheriting from the parent, and will have no handler, resulting in no message being logged to standard output. Tip This can be fixed by either ensuring both has the service value as payment , or simply use the environment variable POWERTOOLS_SERVICE_NAME to ensure service value will be the same across all Loggers when not explicitly set. Overriding Log records \u00b6 You might want to continue to use the same date formatting style, or override location to display the package.function_name:line_number as you previously had. Logger allows you to either change the format or suppress the following keys altogether at the initialization: location , timestamp , level , xray_trace_id . lambda_handler.py We honour standard logging library string formats . 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger date_format = \"%m/ %d /%Y %I:%M:%S %p\" location_format = \"[ %(funcName)s ] %(module)s \" # override location and timestamp format logger = Logger ( service = \"payment\" , location = location_format , datefmt = date_format ) # suppress the location key with a None value logger_two = Logger ( service = \"payment\" , location = None ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"[<module>] lambda_handler\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"02/09/2021 09:25:17 AM\" , \"service\" : \"payment\" } Reordering log keys position \u00b6 You can change the order of standard Logger keys or any keys that will be appended later at runtime via the log_record_order parameter. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger # make message as the first key logger = Logger ( service = \"payment\" , log_record_order = [ \"message\" ]) # make request_id that will be added later as the first key # Logger(service=\"payment\", log_record_order=[\"request_id\"]) # Default key sorting order when omit # Logger(service=\"payment\", log_record_order=[\"level\",\"location\",\"message\",\"timestamp\"]) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"message\" : \"hello world\" , \"level\" : \"INFO\" , \"location\" : \"[<module>]:6\" , \"timestamp\" : \"2021-02-09 09:36:12,280\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 } Setting timestamp to UTC \u00b6 By default, this Logger and standard logging library emits records using local time timestamp. You can override this behaviour via utc parameter: app.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) logger . info ( \"Local time\" ) logger_in_utc = Logger ( service = \"payment\" , utc = True ) logger_in_utc . info ( \"GMT time zone\" ) Custom function for unserializable values \u00b6 By default, Logger uses str to handle values non-serializable by JSON. You can override this behaviour via json_default parameter by passing a Callable: collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger def custom_json_default ( value ): return f \"<non-serializable: { type ( value ) . __name__ } >\" class Unserializable : pass logger = Logger ( service = \"payment\" , json_default = custom_json_default ) def handler ( event , context ): logger . info ( Unserializable ()) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"\" < n o n - serializable : U nser ializable> \"\" , \"timestamp\" : \"2021-05-03 15:17:23,632+0200\" , \"service\" : \"payment\" } Bring your own handler \u00b6 By default, Logger uses StreamHandler and logs to standard output. You can override this behaviour via logger_handler parameter: collect.py 1 2 3 4 5 6 7 8 9 10 import logging from pathlib import Path from aws_lambda_powertools import Logger log_file = Path ( \"/tmp/log.json\" ) log_file_handler = logging . FileHandler ( filename = log_file ) logger = Logger ( service = \"payment\" , logger_handler = log_file_handler ) logger . info ( \"Collecting payment\" ) Bring your own formatter \u00b6 By default, Logger uses LambdaPowertoolsFormatter that persists its custom structure between non-cold start invocations. There could be scenarios where the existing feature set isn't sufficient to your formatting needs. For minor changes like remapping keys after all log record processing has completed, you can override serialize method from LambdaPowertoolsFormatter : custom_formatter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter from typing import Dict class CustomFormatter ( LambdaPowertoolsFormatter ): def serialize ( self , log : Dict ) -> str : \"\"\"Serialize final structured log dict to JSON str\"\"\" log [ \"event\" ] = log . pop ( \"message\" ) # rename message key to event return self . json_serializer ( log ) # use configured json serializer my_formatter = CustomFormatter () logger = Logger ( service = \"example\" , logger_formatter = my_formatter ) logger . info ( \"hello\" ) For replacing the formatter entirely , you can subclass BasePowertoolsFormatter , implement append_keys method, and override format standard logging method. This ensures the current feature set of Logger like injecting Lambda context and sampling will continue to work. Info You might need to implement remove_keys method if you make use of the feature too. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import BasePowertoolsFormatter class CustomFormatter ( BasePowertoolsFormatter ): custom_format = {} # arbitrary dict to hold our structured keys def append_keys ( self , ** additional_keys ): # also used by `inject_lambda_context` decorator self . custom_format . update ( additional_keys ) # Optional unless you make use of this Logger feature def remove_keys ( self , keys : Iterable [ str ]): for key in keys : self . custom_format . pop ( key , None ) def format ( self , record : logging . LogRecord ) -> str : # noqa: A003 \"\"\"Format logging record as structured JSON str\"\"\" return json . dumps ( { \"event\" : super () . format ( record ), \"timestamp\" : self . formatTime ( record ), \"my_default_key\" : \"test\" , ** self . custom_format , } ) logger = Logger ( service = \"payment\" , logger_formatter = CustomFormatter ()) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 { \"event\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494\" , \"my_default_key\" : \"test\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } Bring your own JSON serializer \u00b6 By default, Logger uses json.dumps and json.loads as serializer and deserializer respectively. There could be scenarios where you are making use of alternative JSON libraries like orjson . As parameters don't always translate well between them, you can pass any callable that receives a Dict and return a str : collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import orjson from aws_lambda_powertools import Logger custom_serializer = orjson . dumps custom_deserializer = orjson . loads logger = Logger ( service = \"payment\" , json_serializer = custom_serializer , json_deserializer = custom_deserializer ) # when using parameters, you can pass a partial # custom_serializer=functools.partial(orjson.dumps, option=orjson.OPT_SERIALIZE_NUMPY) Built-in Correlation ID expressions \u00b6 You can use any of the following built-in JMESPath expressions as part of inject_lambda_context decorator . Escaping necessary for the - character Any object key named with - must be escaped, for example request.headers.\"x-amzn-trace-id\" . Name Expression Description API_GATEWAY_REST \"requestContext.requestId\" API Gateway REST API request ID API_GATEWAY_HTTP \"requestContext.requestId\" API Gateway HTTP API request ID APPSYNC_RESOLVER 'request.headers.\"x-amzn-trace-id\"' AppSync X-Ray Trace ID APPLICATION_LOAD_BALANCER 'headers.\"x-amzn-trace-id\"' ALB X-Ray Trace ID EVENT_BRIDGE \"id\" EventBridge Event ID Testing your code \u00b6 Inject Lambda Context \u00b6 When unit testing your code that makes use of inject_lambda_context decorator, you need to pass a dummy Lambda Context, or else Logger will fail. This is a Pytest sample that provides the minimum information necessary for Logger to succeed: fake_lambda_context_for_logger.py Note that dataclasses are available in Python 3.7+ only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dataclasses import dataclass import pytest @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } your_lambda_handler ( test_event , lambda_context ) # this will now have a Context object populated fake_lambda_context_for_logger_py36.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from collections import namedtuple import pytest @pytest . fixture def lambda_context (): lambda_context = { \"function_name\" : \"test\" , \"memory_limit_in_mb\" : 128 , \"invoked_function_arn\" : \"arn:aws:lambda:eu-west-1:809313241:function:test\" , \"aws_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , } return namedtuple ( \"LambdaContext\" , lambda_context . keys ())( * lambda_context . values ()) def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } # this will now have a Context object populated your_lambda_handler ( test_event , lambda_context ) Tip If you're using pytest and are looking to assert plain log messages, do check out the built-in caplog fixture . Pytest live log feature \u00b6 Pytest Live Log feature duplicates emitted log messages in order to style log statements according to their levels, for this to work use POWERTOOLS_LOG_DEDUPLICATION_DISABLED env var. 1 POWERTOOLS_LOG_DEDUPLICATION_DISABLED = \"1\" pytest -o log_cli = 1 Warning This feature should be used with care, as it explicitly disables our ability to filter propagated messages to the root logger (if configured). FAQ \u00b6 How can I enable boto3 and botocore library logging? You can enable the botocore and boto3 logs by using the set_stream_logger method, this method will add a stream handler for the given name and level to the logging module. By default, this logs all boto3 messages to stdout. log_botocore_and_boto3.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict , List from aws_lambda_powertools.utilities.typing import LambdaContext from aws_lambda_powertools import Logger import boto3 boto3 . set_stream_logger () boto3 . set_stream_logger ( 'botocore' ) logger = Logger () client = boto3 . client ( 's3' ) def handler ( event : Dict , context : LambdaContext ) -> List : response = client . list_buckets () return response . get ( \"Buckets\" , []) What's the difference between append_keys and extra ? Keys added with append_keys will persist across multiple log messages while keys added via extra will only be available in a given log message operation. Here's an example where we persist payment_id not request_id . Note that payment_id remains in both log messages while booking_id is only available in the first message. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( payment_id = \"123456789\" ) try : booking_id = book_flight () logger . info ( \"Flight booked successfully\" , extra = { \"booking_id\" : booking_id }) except BookingReservationError : ... logger . info ( \"goodbye\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"level\" : \"INFO\" , \"location\" : \"<module>:10\" , \"message\" : \"Flight booked successfully\" , \"timestamp\" : \"2021-01-12 14:09:10,859\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" , \"booking_id\" : \"75edbad0-0857-4fc9-b547-6180e2f7959b\" }, { \"level\" : \"INFO\" , \"location\" : \"<module>:14\" , \"message\" : \"goodbye\" , \"timestamp\" : \"2021-01-12 14:09:10,860\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" } How do I aggregate and search Powertools logs across accounts? As of now, ElasticSearch (ELK) or 3rd party solutions are best suited to this task. Please see this discussion for more information: https://github.com/awslabs/aws-lambda-powertools-python/issues/460","title":"Logger"},{"location":"core/logger/#key-features","text":"Capture key fields from Lambda context, cold start and structures logging output as JSON Log Lambda event when instructed (disabled by default) Log sampling enables DEBUG log level for a percentage of requests (disabled by default) Append additional keys to structured log at any point in time","title":"Key features"},{"location":"core/logger/#getting-started","text":"Logger requires two settings: Setting Description Environment variable Constructor parameter Logging level Sets how verbose Logger should be (INFO, by default) LOG_LEVEL level Service Sets service key that will be present across all log statements POWERTOOLS_SERVICE_NAME service Example using AWS Serverless Application Model (SAM) template.yaml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : example Function.cs 1 2 3 using AWS.Lambda.PowerTools.Logging // TODO: finish the initialisation sample","title":"Getting started"},{"location":"core/logger/#standard-structured-keys","text":"Your Logger will include the following keys to your structured logging: Key Example Note level : str INFO Logging level location : str collect.handler:1 Source code location where statement was executed message : Any Collecting payment Unserializable JSON values are casted as str timestamp : str 2021-05-03 10:20:19,650+0200 Timestamp with milliseconds, by default uses local timezone service : str payment Service name defined, by default service_undefined xray_trace_id : str 1-5759e988-bd862e3fe1be46a994272793 When tracing is enabled , it shows X-Ray Trace ID sampling_rate : float 0.1 When enabled, it shows sampling rate in percentage e.g. 10% exception_name : str ValueError When logger.exception is used and there is an exception exception : str Traceback (most recent call last).. When logger.exception is used and there is an exception","title":"Standard structured keys"},{"location":"core/logger/#capturing-lambda-context-info","text":"You can enrich your structured logs with key Lambda context information via inject_lambda_context . Function.cs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 using AWS.Lambda.PowerTools.Logging logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) # You can log entire objects too logger . info ({ \"operation\" : \"collect_payment\" , \"charge_id\" : event [ ' charge_id ' ] }) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : { \"operation\" : \"collect_payment\" , \"charge_id\" : \"ch_AZFlk2345C0\" }, \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } When used, this will include the following keys: Key Example cold_start : bool false function_name str example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_memory_size : int 128 function_arn : str arn:aws:lambda:eu-west-1:012345678910:function:example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_request_id : str 899856cb-83d1-40d7-8611-9e78f15f32f4","title":"Capturing Lambda context info"},{"location":"core/logger/#logging-incoming-event","text":"When debugging in non-production environments, you can instruct Logger to log the incoming event with log_event param or via POWERTOOLS_LOGGER_LOG_EVENT env var. Warning This is disabled by default to prevent sensitive info being logged. log_handler_event.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( log_event = True ) def handler ( event , context ): ...","title":"Logging incoming event"},{"location":"core/logger/#setting-a-correlation-id","text":"You can set a Correlation ID using correlation_id_path param by passing a JMESPath expression . You can retrieve correlation IDs via get_correlation_id method collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = \"headers.my_request_id_header\" ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"headers\" : { \"my_request_id_header\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" } We provide built-in JMESPath expressions for known event sources, where either a request ID or X-Ray Trace ID are present. collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" }","title":"Setting a Correlation ID"},{"location":"core/logger/#appending-additional-keys","text":"Custom keys are persisted across warm invocations Always set additional keys as part of your handler to ensure they have the latest value, or explicitly clear them with clear_state=True . You can append additional keys using either mechanism: Persist new keys across all future log messages via append_keys method Add additional keys on a per log message basis via extra parameter","title":"Appending additional keys"},{"location":"core/logger/#append_keys-method","text":"NOTE: append_keys replaces structure_logs(append=True, **kwargs) method. Both will continue to work until the next major version. You can append your own keys to your existing Logger via append_keys(**additional_key_values) method. collect.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): order_id = event . get ( \"order_id\" ) # this will ensure order_id key always has the latest value before logging logger . append_keys ( order_id = order_id ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:11\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"order_id\" : \"order_id_value\" } Logger will automatically reject any key with a None value If you conditionally add keys depending on the payload, you can follow the example above. This example will add order_id if its value is not empty, and in subsequent invocations where order_id might not be present it'll remove it from the Logger.","title":"append_keys method"},{"location":"core/logger/#extra-parameter","text":"Extra parameter is available for all log levels' methods, as implemented in the standard logging library - e.g. logger.info, logger.warning . It accepts any dictionary, and all keyword arguments will be added as part of the root structure of the logs for that log statement. Any keyword argument added using extra will not be persisted for subsequent messages. extra_parameter.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) fields = { \"request_id\" : \"1123\" } logger . info ( \"Collecting payment\" , extra = fields ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:6\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"request_id\" : \"1123\" }","title":"extra parameter"},{"location":"core/logger/#set_correlation_id-method","text":"You can set a correlation_id to your existing Logger via set_correlation_id(value) method by passing any string value. collect.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . set_correlation_id ( event [ \"requestContext\" ][ \"requestId\" ]) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"correlation_id\" : \"correlation_id_value\" } Alternatively, you can combine Data Classes utility with Logger to use dot notation object: collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent logger = Logger ( service = \"payment\" ) def handler ( event , context ): event = APIGatewayProxyEvent ( event ) logger . set_correlation_id ( event . request_context . request_id ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:9\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" }","title":"set_correlation_id method"},{"location":"core/logger/#removing-additional-keys","text":"You can remove any additional key from Logger state using remove_keys . collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( sample_key = \"value\" ) logger . info ( \"Collecting payment\" ) logger . remove_keys ([ \"sample_key\" ]) logger . info ( \"Collecting payment without sample key\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"sample_key\" : \"value\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment without sample key\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" }","title":"Removing additional keys"},{"location":"core/logger/#clearing-all-state","text":"Logger is commonly initialized in the global scope. Due to Lambda Execution Context reuse , this means that custom keys can be persisted across invocations. If you want all custom keys to be deleted, you can use clear_state=True param in inject_lambda_context decorator. Info This is useful when you add multiple custom keys conditionally, instead of setting a default None value if not present. Any key with None value is automatically removed by Logger. This can have unintended side effects if you use Layers Lambda Layers code is imported before the Lambda handler. This means that clear_state=True will instruct Logger to remove any keys previously added before Lambda handler execution proceeds. You can either avoid running any code as part of Lambda Layers global scope, or override keys with their latest value as part of handler's execution. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( clear_state = True ) def handler ( event , context ): if event . get ( \"special_key\" ): # Should only be available in the first request log # as the second request doesn't contain `special_key` logger . append_keys ( debugging_key = \"value\" ) logger . info ( \"Collecting payment\" ) #1 request 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"special_key\" : \"debug_key\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } #2 request 1 2 3 4 5 6 7 8 9 10 11 12 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : false , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }","title":"Clearing all state"},{"location":"core/logger/#logging-exceptions","text":"Use logger.exception method to log contextual information about exceptions. Logger will include exception_name and exception keys to aid troubleshooting and error enumeration. Tip You can use your preferred Log Analytics tool to enumerate and visualize exceptions across all your services using exception_name key. collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) try : raise ValueError ( \"something went wrong\" ) except Exception : logger . exception ( \"Received an exception\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"level\" : \"ERROR\" , \"location\" : \"collect.handler:5\" , \"message\" : \"Received an exception\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"exception_name\" : \"ValueError\" , \"exception\" : \"Traceback (most recent call last):\\n File \\\"<input>\\\", line 2, in <module>\\nValueError: something went wrong\" }","title":"Logging exceptions"},{"location":"core/logger/#advanced","text":"","title":"Advanced"},{"location":"core/logger/#reusing-logger-across-your-code","text":"Logger supports inheritance via child parameter. This allows you to create multiple Loggers across your code base, and propagate changes such as new keys to all Loggers. collect.py 1 2 3 4 5 6 7 8 import shared # Creates a child logger named \"payment.shared\" from aws_lambda_powertools import Logger logger = Logger () # POWERTOOLS_SERVICE_NAME: \"payment\" def handler ( event , context ): shared . inject_payment_id ( event ) ... shared.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( child = True ) # POWERTOOLS_SERVICE_NAME: \"payment\" def inject_payment_id ( event ): logger . structure_logs ( append = True , payment_id = event . get ( \"payment_id\" )) In this example, Logger will create a parent logger named payment and a child logger named payment.shared . Changes in either parent or child logger will be propagated bi-directionally. Child loggers will be named after the following convention {service}.{filename} If you forget to use child param but the service name is the same of the parent, we will return the existing parent Logger instead.","title":"Reusing Logger across your code"},{"location":"core/logger/#sampling-debug-logs","text":"Use sampling when you want to dynamically change your log level to DEBUG based on a percentage of your concurrent/cold start invocations . You can use values ranging from 0.0 to 1 (100%) when setting POWERTOOLS_LOGGER_SAMPLE_RATE env var or sample_rate parameter in Logger. When is this useful? Let's imagine a sudden spike increase in concurrency triggered a transient issue downstream. When looking into the logs you might not have enough information, and while you can adjust log levels it might not happen again. This feature takes into account transient issues where additional debugging information can be useful. Sampling decision happens at the Logger initialization. This means sampling may happen significantly more or less than depending on your traffic patterns, for example a steady low number of invocations and thus few cold starts. Note If you want Logger to calculate sampling upon every invocation, please open a feature request . collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger # Sample 10% of debug logs e.g. 0.1 logger = Logger ( service = \"payment\" , sample_rate = 0.1 ) def handler ( event , context ): logger . debug ( \"Verifying whether order_id is present\" ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Verifying whether order_id is present\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 }","title":"Sampling debug logs"},{"location":"core/logger/#lambdapowertoolsformatter","text":"Logger propagates a few formatting configurations to the built-in LambdaPowertoolsFormatter logging formatter. If you prefer configuring it separately, or you'd want to bring this JSON Formatter to another application, these are the supported settings: Parameter Description Default json_serializer function to serialize obj to a JSON formatted str json.dumps json_deserializer function to deserialize str , bytes , bytearray containing a JSON document to a Python obj json.loads json_default function to coerce unserializable values, when no custom serializer/deserializer is set str datefmt string directives (strftime) to format log timestamp %Y-%m-%d %H:%M:%S,%F%z , where %F is a custom ms directive utc set logging timestamp to UTC False log_record_order set order of log keys when logging [\"level\", \"location\", \"message\", \"timestamp\"] kwargs key-value to be included in log messages None LambdaPowertoolsFormatter.py 1 2 3 4 5 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter formatter = LambdaPowertoolsFormatter ( utc = True , log_record_order = [ \"message\" ]) logger = Logger ( service = \"example\" , logger_formatter = formatter )","title":"LambdaPowertoolsFormatter"},{"location":"core/logger/#migrating-from-other-loggers","text":"If you're migrating from other Loggers, there are few key points to be aware of: Service parameter , Inheriting Loggers , Overriding Log records , and Logging exceptions .","title":"Migrating from other Loggers"},{"location":"core/logger/#the-service-parameter","text":"Service is what defines the Logger name, including what the Lambda function is responsible for, or part of (e.g payment service). For Logger, the service is the logging key customers can use to search log operations for one or more functions - For example, search for all errors, or messages like X, where service is payment .","title":"The service parameter"},{"location":"core/logger/#inheriting-loggers","text":"Python Logging hierarchy happens via the dot notation: service , service.child , service.child_2 For inheritance, Logger uses a child=True parameter along with service being the same value across Loggers. For child Loggers, we introspect the name of your module where Logger(child=True, service=\"name\") is called, and we name your Logger as {service}.{filename} . Danger A common issue when migrating from other Loggers is that service might be defined in the parent Logger (no child param), and not defined in the child Logger: incorrect_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( child = True ) correct_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" , child = True ) In this case, Logger will register a Logger named payment , and a Logger named service_undefined . The latter isn't inheriting from the parent, and will have no handler, resulting in no message being logged to standard output. Tip This can be fixed by either ensuring both has the service value as payment , or simply use the environment variable POWERTOOLS_SERVICE_NAME to ensure service value will be the same across all Loggers when not explicitly set.","title":"Inheriting Loggers"},{"location":"core/logger/#overriding-log-records","text":"You might want to continue to use the same date formatting style, or override location to display the package.function_name:line_number as you previously had. Logger allows you to either change the format or suppress the following keys altogether at the initialization: location , timestamp , level , xray_trace_id . lambda_handler.py We honour standard logging library string formats . 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger date_format = \"%m/ %d /%Y %I:%M:%S %p\" location_format = \"[ %(funcName)s ] %(module)s \" # override location and timestamp format logger = Logger ( service = \"payment\" , location = location_format , datefmt = date_format ) # suppress the location key with a None value logger_two = Logger ( service = \"payment\" , location = None ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"[<module>] lambda_handler\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"02/09/2021 09:25:17 AM\" , \"service\" : \"payment\" }","title":"Overriding Log records"},{"location":"core/logger/#reordering-log-keys-position","text":"You can change the order of standard Logger keys or any keys that will be appended later at runtime via the log_record_order parameter. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger # make message as the first key logger = Logger ( service = \"payment\" , log_record_order = [ \"message\" ]) # make request_id that will be added later as the first key # Logger(service=\"payment\", log_record_order=[\"request_id\"]) # Default key sorting order when omit # Logger(service=\"payment\", log_record_order=[\"level\",\"location\",\"message\",\"timestamp\"]) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"message\" : \"hello world\" , \"level\" : \"INFO\" , \"location\" : \"[<module>]:6\" , \"timestamp\" : \"2021-02-09 09:36:12,280\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 }","title":"Reordering log keys position"},{"location":"core/logger/#setting-timestamp-to-utc","text":"By default, this Logger and standard logging library emits records using local time timestamp. You can override this behaviour via utc parameter: app.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) logger . info ( \"Local time\" ) logger_in_utc = Logger ( service = \"payment\" , utc = True ) logger_in_utc . info ( \"GMT time zone\" )","title":"Setting timestamp to UTC"},{"location":"core/logger/#custom-function-for-unserializable-values","text":"By default, Logger uses str to handle values non-serializable by JSON. You can override this behaviour via json_default parameter by passing a Callable: collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger def custom_json_default ( value ): return f \"<non-serializable: { type ( value ) . __name__ } >\" class Unserializable : pass logger = Logger ( service = \"payment\" , json_default = custom_json_default ) def handler ( event , context ): logger . info ( Unserializable ()) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"\" < n o n - serializable : U nser ializable> \"\" , \"timestamp\" : \"2021-05-03 15:17:23,632+0200\" , \"service\" : \"payment\" }","title":"Custom function for unserializable values"},{"location":"core/logger/#bring-your-own-handler","text":"By default, Logger uses StreamHandler and logs to standard output. You can override this behaviour via logger_handler parameter: collect.py 1 2 3 4 5 6 7 8 9 10 import logging from pathlib import Path from aws_lambda_powertools import Logger log_file = Path ( \"/tmp/log.json\" ) log_file_handler = logging . FileHandler ( filename = log_file ) logger = Logger ( service = \"payment\" , logger_handler = log_file_handler ) logger . info ( \"Collecting payment\" )","title":"Bring your own handler"},{"location":"core/logger/#bring-your-own-formatter","text":"By default, Logger uses LambdaPowertoolsFormatter that persists its custom structure between non-cold start invocations. There could be scenarios where the existing feature set isn't sufficient to your formatting needs. For minor changes like remapping keys after all log record processing has completed, you can override serialize method from LambdaPowertoolsFormatter : custom_formatter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter from typing import Dict class CustomFormatter ( LambdaPowertoolsFormatter ): def serialize ( self , log : Dict ) -> str : \"\"\"Serialize final structured log dict to JSON str\"\"\" log [ \"event\" ] = log . pop ( \"message\" ) # rename message key to event return self . json_serializer ( log ) # use configured json serializer my_formatter = CustomFormatter () logger = Logger ( service = \"example\" , logger_formatter = my_formatter ) logger . info ( \"hello\" ) For replacing the formatter entirely , you can subclass BasePowertoolsFormatter , implement append_keys method, and override format standard logging method. This ensures the current feature set of Logger like injecting Lambda context and sampling will continue to work. Info You might need to implement remove_keys method if you make use of the feature too. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import BasePowertoolsFormatter class CustomFormatter ( BasePowertoolsFormatter ): custom_format = {} # arbitrary dict to hold our structured keys def append_keys ( self , ** additional_keys ): # also used by `inject_lambda_context` decorator self . custom_format . update ( additional_keys ) # Optional unless you make use of this Logger feature def remove_keys ( self , keys : Iterable [ str ]): for key in keys : self . custom_format . pop ( key , None ) def format ( self , record : logging . LogRecord ) -> str : # noqa: A003 \"\"\"Format logging record as structured JSON str\"\"\" return json . dumps ( { \"event\" : super () . format ( record ), \"timestamp\" : self . formatTime ( record ), \"my_default_key\" : \"test\" , ** self . custom_format , } ) logger = Logger ( service = \"payment\" , logger_formatter = CustomFormatter ()) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 { \"event\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494\" , \"my_default_key\" : \"test\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }","title":"Bring your own formatter"},{"location":"core/logger/#bring-your-own-json-serializer","text":"By default, Logger uses json.dumps and json.loads as serializer and deserializer respectively. There could be scenarios where you are making use of alternative JSON libraries like orjson . As parameters don't always translate well between them, you can pass any callable that receives a Dict and return a str : collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import orjson from aws_lambda_powertools import Logger custom_serializer = orjson . dumps custom_deserializer = orjson . loads logger = Logger ( service = \"payment\" , json_serializer = custom_serializer , json_deserializer = custom_deserializer ) # when using parameters, you can pass a partial # custom_serializer=functools.partial(orjson.dumps, option=orjson.OPT_SERIALIZE_NUMPY)","title":"Bring your own JSON serializer"},{"location":"core/logger/#built-in-correlation-id-expressions","text":"You can use any of the following built-in JMESPath expressions as part of inject_lambda_context decorator . Escaping necessary for the - character Any object key named with - must be escaped, for example request.headers.\"x-amzn-trace-id\" . Name Expression Description API_GATEWAY_REST \"requestContext.requestId\" API Gateway REST API request ID API_GATEWAY_HTTP \"requestContext.requestId\" API Gateway HTTP API request ID APPSYNC_RESOLVER 'request.headers.\"x-amzn-trace-id\"' AppSync X-Ray Trace ID APPLICATION_LOAD_BALANCER 'headers.\"x-amzn-trace-id\"' ALB X-Ray Trace ID EVENT_BRIDGE \"id\" EventBridge Event ID","title":"Built-in Correlation ID expressions"},{"location":"core/logger/#testing-your-code","text":"","title":"Testing your code"},{"location":"core/logger/#inject-lambda-context","text":"When unit testing your code that makes use of inject_lambda_context decorator, you need to pass a dummy Lambda Context, or else Logger will fail. This is a Pytest sample that provides the minimum information necessary for Logger to succeed: fake_lambda_context_for_logger.py Note that dataclasses are available in Python 3.7+ only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dataclasses import dataclass import pytest @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } your_lambda_handler ( test_event , lambda_context ) # this will now have a Context object populated fake_lambda_context_for_logger_py36.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from collections import namedtuple import pytest @pytest . fixture def lambda_context (): lambda_context = { \"function_name\" : \"test\" , \"memory_limit_in_mb\" : 128 , \"invoked_function_arn\" : \"arn:aws:lambda:eu-west-1:809313241:function:test\" , \"aws_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , } return namedtuple ( \"LambdaContext\" , lambda_context . keys ())( * lambda_context . values ()) def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } # this will now have a Context object populated your_lambda_handler ( test_event , lambda_context ) Tip If you're using pytest and are looking to assert plain log messages, do check out the built-in caplog fixture .","title":"Inject Lambda Context"},{"location":"core/logger/#pytest-live-log-feature","text":"Pytest Live Log feature duplicates emitted log messages in order to style log statements according to their levels, for this to work use POWERTOOLS_LOG_DEDUPLICATION_DISABLED env var. 1 POWERTOOLS_LOG_DEDUPLICATION_DISABLED = \"1\" pytest -o log_cli = 1 Warning This feature should be used with care, as it explicitly disables our ability to filter propagated messages to the root logger (if configured).","title":"Pytest live log feature"},{"location":"core/logger/#faq","text":"How can I enable boto3 and botocore library logging? You can enable the botocore and boto3 logs by using the set_stream_logger method, this method will add a stream handler for the given name and level to the logging module. By default, this logs all boto3 messages to stdout. log_botocore_and_boto3.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict , List from aws_lambda_powertools.utilities.typing import LambdaContext from aws_lambda_powertools import Logger import boto3 boto3 . set_stream_logger () boto3 . set_stream_logger ( 'botocore' ) logger = Logger () client = boto3 . client ( 's3' ) def handler ( event : Dict , context : LambdaContext ) -> List : response = client . list_buckets () return response . get ( \"Buckets\" , []) What's the difference between append_keys and extra ? Keys added with append_keys will persist across multiple log messages while keys added via extra will only be available in a given log message operation. Here's an example where we persist payment_id not request_id . Note that payment_id remains in both log messages while booking_id is only available in the first message. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( payment_id = \"123456789\" ) try : booking_id = book_flight () logger . info ( \"Flight booked successfully\" , extra = { \"booking_id\" : booking_id }) except BookingReservationError : ... logger . info ( \"goodbye\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"level\" : \"INFO\" , \"location\" : \"<module>:10\" , \"message\" : \"Flight booked successfully\" , \"timestamp\" : \"2021-01-12 14:09:10,859\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" , \"booking_id\" : \"75edbad0-0857-4fc9-b547-6180e2f7959b\" }, { \"level\" : \"INFO\" , \"location\" : \"<module>:14\" , \"message\" : \"goodbye\" , \"timestamp\" : \"2021-01-12 14:09:10,860\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" } How do I aggregate and search Powertools logs across accounts? As of now, ElasticSearch (ELK) or 3rd party solutions are best suited to this task. Please see this discussion for more information: https://github.com/awslabs/aws-lambda-powertools-python/issues/460","title":"FAQ"},{"location":"core/metrics/","text":"Metrics creates custom metrics asynchronously by logging metrics to standard output following Amazon CloudWatch Embedded Metric Format (EMF) . These metrics can be visualized through Amazon CloudWatch Console . Key features \u00b6 Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob) Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc) Metrics are created asynchronously by CloudWatch service, no custom stacks needed Context manager to create a one off metric with a different dimension Terminologies \u00b6 If you're new to Amazon CloudWatch, there are two terminologies you must be aware of before using this utility: Namespace . It's the highest level container that will group multiple metrics from multiple services for a given application, for example ServerlessEcommerce . Dimensions . Metrics metadata in key-value format. They help you slice and dice metrics visualization, for example ColdStart metric by Payment service . Metric terminology, visually explained Getting started \u00b6 Metric has two global settings that will be used across all metrics emitted: Setting Description Environment variable Constructor parameter Metric namespace Logical container where all metrics will be placed e.g. ServerlessAirline POWERTOOLS_METRICS_NAMESPACE namespace Service Optionally, sets service metric dimension across all metrics e.g. payment POWERTOOLS_SERVICE_NAME service Use your application or main service as the metric namespace to easily group all metrics Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : POWERTOOLS_SERVICE_NAME : payment POWERTOOLS_METRICS_NAMESPACE : ServerlessAirline app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics () # Sets metric namespace and service via env var # OR metrics = Metrics ( namespace = \"ServerlessAirline\" , service = \"orders\" ) # Sets metric namespace, and service as a metric dimension You can initialize Metrics anywhere in your code - It'll keep track of your aggregate metrics in memory. Creating metrics \u00b6 You can create metrics using add_metric , and you can create dimensions for all your aggregate metrics using add_dimension method. Metrics 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Metrics with custom dimensions 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Autocomplete Metric Units MetricUnit enum facilitate finding a supported metric unit by CloudWatch. Alternatively, you can pass the value as a string if you already know them e.g. \"Count\". Metrics overflow CloudWatch EMF supports a max of 100 metrics per batch. Metrics utility will flush all metrics when adding the 100th metric. Subsequent metrics, e.g. 101th, will be aggregated into a new EMF object, for your convenience. Do not create metrics or dimensions outside the handler Metrics or dimensions added in the global scope will only be added during cold start. Disregard if you that's the intended behaviour. Adding default dimensions \u00b6 You can use either set_default_dimensions method or default_permissions parameter in log_metrics decorator to persist dimensions across Lambda invocations. If you'd like to remove them at some point, you can use clear_default_dimensions method. set_default_dimensions method 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . set_default_dimensions ( environment = \"prod\" , another = \"one\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) with log_metrics decorator 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) DEFAULT_DIMENSIONS = { \"environment\" : \"prod\" , \"another\" : \"one\" } @metrics . log_metrics ( default_dimensions = DEFAULT_DIMENSIONS ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Flushing metrics \u00b6 As you finish adding all your metrics, you need to serialize and flush them to standard output. You can do that automatically with the log_metrics decorator. This decorator also validates , serializes , and flushes all your metrics. During metrics validation, if no metrics are provided then a warning will be logged, but no exception will be raised. app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"ExampleService\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"BookingConfirmation\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"BookingConfirmation\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"ExampleService\" } Metric validation If metrics are provided, and any of the following criteria are not met, SchemaValidationError exception will be raised: Maximum of 9 dimensions Namespace is set, and no more than one Metric units must be supported by CloudWatch Raising SchemaValidationError on empty metrics \u00b6 If you want to ensure that at least one metric is emitted, you can pass raise_on_empty_metrics to the log_metrics decorator: app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.metrics import Metrics metrics = Metrics () @metrics . log_metrics ( raise_on_empty_metrics = True ) def lambda_handler ( evt , ctx ): ... Suppressing warning messages on empty metrics If you expect your function to execute without publishing metrics every time, you can suppress the warning with warnings.filterwarnings(\"ignore\", \"No metrics to publish*\") . Nesting multiple middlewares \u00b6 When using multiple middlewares, use log_metrics as your last decorator wrapping all subsequent ones to prevent early Metric validations when code hasn't been run yet. nested_middlewares.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Metrics , Tracer from aws_lambda_powertools.metrics import MetricUnit tracer = Tracer ( service = \"booking\" ) metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics @tracer . capture_lambda_handler def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) Capturing cold start metric \u00b6 You can optionally capture cold start metrics with log_metrics decorator via capture_cold_start_metric param. app.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Metrics metrics = Metrics ( service = \"ExampleService\" ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): ... If it's a cold start invocation, this feature will: Create a separate EMF blob solely containing a metric named ColdStart Add function_name and service dimensions This has the advantage of keeping cold start metric separate from your application metrics, where you might have unrelated dimensions. We do not emit 0 as a value for ColdStart metric for cost reasons. Let us know if you'd prefer a flag to override it Advanced \u00b6 Adding metadata \u00b6 You can add high-cardinality data as part of your Metrics log with add_metadata method. This is useful when you want to search highly contextual information along with your metrics in your logs. Info This will not be available during metrics visualization - Use dimensions for this purpose app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_metadata ( key = \"booking_id\" , value = \"booking_uuid\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"SuccessfulBooking\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"SuccessfulBooking\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"booking\" , \"booking_id\" : \"booking_uuid\" } Single metric with a different dimension \u00b6 CloudWatch EMF uses the same dimensions across all your metrics. Use single_metric if you have a metric that should have different dimensions. Info Generally, this would be an edge case since you pay for unique metric . Keep the following formula in mind: unique metric = (metric_name + dimension_name + dimension_value) single_metric.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import single_metric from aws_lambda_powertools.metrics import MetricUnit def lambda_handler ( evt , ctx ): with single_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 , namespace = \"ExampleApplication\" ) as metric : metric . add_dimension ( name = \"function_context\" , value = \"$LATEST\" ) ... Flushing metrics manually \u00b6 If you prefer not to use log_metrics because you might want to encapsulate additional logic when doing so, you can manually flush and clear metrics as follows: Warning Metrics, dimensions and namespace validation still applies. manual_metric_serialization.py 1 2 3 4 5 6 7 8 9 10 11 import json from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 ) your_metrics_object = metrics . serialize_metric_set () metrics . clear_metrics () print ( json . dumps ( your_metrics_object )) Testing your code \u00b6 Environment variables \u00b6 Use POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME env vars when unit testing your code to ensure metric namespace and dimension objects are created, and your code doesn't fail validation. 1 POWERTOOLS_SERVICE_NAME = \"Example\" POWERTOOLS_METRICS_NAMESPACE = \"Application\" python -m pytest If you prefer setting environment variable for specific tests, and are using Pytest, you can use monkeypatch fixture: pytest_env_var.py 1 2 3 4 5 6 def test_namespace_env_var ( monkeypatch ): # Set POWERTOOLS_METRICS_NAMESPACE before initializating Metrics monkeypatch . setenv ( \"POWERTOOLS_METRICS_NAMESPACE\" , namespace ) metrics = Metrics () ... Ignore this, if you are explicitly setting namespace/default dimension via namespace and service parameters: metrics = Metrics(namespace=ApplicationName, service=ServiceName) Clearing metrics \u00b6 Metrics keep metrics in memory across multiple instances. If you need to test this behaviour, you can use the following Pytest fixture to ensure metrics are reset incl. cold start: pytest_metrics_reset_fixture.py 1 2 3 4 5 6 7 8 @pytest . fixture ( scope = \"function\" , autouse = True ) def reset_metric_set (): # Clear out every metric data prior to every test metrics = Metrics () metrics . clear_metrics () metrics_global . is_cold_start = True # ensure each test has cold start metrics . clear_default_dimensions () # remove persisted default dimensions, if any yield Functional testing \u00b6 As metrics are logged to standard output, you can read standard output and assert whether metrics are present. Here's an example using pytest with capsys built-in fixture: Assert single EMF blob with pytest.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit import json def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN we utilize log_metrics to serialize # and flush all metrics at the end of a function execution @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) lambda_handler ({}, {}) log = capsys . readouterr () . out . strip () # remove any extra line metrics_output = json . loads ( log ) # deserialize JSON str # THEN we should have no exceptions # and a valid EMF object should be flushed correctly assert \"SuccessfulBooking\" in log # basic string assertion in JSON str assert \"SuccessfulBooking\" in metrics_output [ \"_aws\" ][ \"CloudWatchMetrics\" ][ 0 ][ \"Metrics\" ][ 0 ][ \"Name\" ] Assert multiple EMF blobs with pytest 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit from collections import namedtuple import json def capture_metrics_output_multiple_emf_objects ( capsys ): return [ json . loads ( line . strip ()) for line in capsys . readouterr () . out . split ( \" \\n \" ) if line ] def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN log_metrics is used with capture_cold_start_metric @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) # log_metrics uses function_name property from context to add as a dimension for cold start metric LambdaContext = namedtuple ( \"LambdaContext\" , \"function_name\" ) lambda_handler ({}, LambdaContext ( \"example_fn\" ) cold_start_blob , custom_metrics_blob = capture_metrics_output_multiple_emf_objects ( capsys ) # THEN ColdStart metric and function_name dimension should be logged # in a separate EMF blob than the application metrics assert cold_start_blob [ \"ColdStart\" ] == [ 1.0 ] assert cold_start_blob [ \"function_name\" ] == \"example_fn\" assert \"SuccessfulBooking\" in custom_metrics_blob # as per previous example For more elaborate assertions and comparisons, check out our functional testing for Metrics utility","title":"Metrics"},{"location":"core/metrics/#key-features","text":"Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob) Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc) Metrics are created asynchronously by CloudWatch service, no custom stacks needed Context manager to create a one off metric with a different dimension","title":"Key features"},{"location":"core/metrics/#terminologies","text":"If you're new to Amazon CloudWatch, there are two terminologies you must be aware of before using this utility: Namespace . It's the highest level container that will group multiple metrics from multiple services for a given application, for example ServerlessEcommerce . Dimensions . Metrics metadata in key-value format. They help you slice and dice metrics visualization, for example ColdStart metric by Payment service . Metric terminology, visually explained","title":"Terminologies"},{"location":"core/metrics/#getting-started","text":"Metric has two global settings that will be used across all metrics emitted: Setting Description Environment variable Constructor parameter Metric namespace Logical container where all metrics will be placed e.g. ServerlessAirline POWERTOOLS_METRICS_NAMESPACE namespace Service Optionally, sets service metric dimension across all metrics e.g. payment POWERTOOLS_SERVICE_NAME service Use your application or main service as the metric namespace to easily group all metrics Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : POWERTOOLS_SERVICE_NAME : payment POWERTOOLS_METRICS_NAMESPACE : ServerlessAirline app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics () # Sets metric namespace and service via env var # OR metrics = Metrics ( namespace = \"ServerlessAirline\" , service = \"orders\" ) # Sets metric namespace, and service as a metric dimension You can initialize Metrics anywhere in your code - It'll keep track of your aggregate metrics in memory.","title":"Getting started"},{"location":"core/metrics/#creating-metrics","text":"You can create metrics using add_metric , and you can create dimensions for all your aggregate metrics using add_dimension method. Metrics 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Metrics with custom dimensions 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Autocomplete Metric Units MetricUnit enum facilitate finding a supported metric unit by CloudWatch. Alternatively, you can pass the value as a string if you already know them e.g. \"Count\". Metrics overflow CloudWatch EMF supports a max of 100 metrics per batch. Metrics utility will flush all metrics when adding the 100th metric. Subsequent metrics, e.g. 101th, will be aggregated into a new EMF object, for your convenience. Do not create metrics or dimensions outside the handler Metrics or dimensions added in the global scope will only be added during cold start. Disregard if you that's the intended behaviour.","title":"Creating metrics"},{"location":"core/metrics/#adding-default-dimensions","text":"You can use either set_default_dimensions method or default_permissions parameter in log_metrics decorator to persist dimensions across Lambda invocations. If you'd like to remove them at some point, you can use clear_default_dimensions method. set_default_dimensions method 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . set_default_dimensions ( environment = \"prod\" , another = \"one\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) with log_metrics decorator 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) DEFAULT_DIMENSIONS = { \"environment\" : \"prod\" , \"another\" : \"one\" } @metrics . log_metrics ( default_dimensions = DEFAULT_DIMENSIONS ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 )","title":"Adding default dimensions"},{"location":"core/metrics/#flushing-metrics","text":"As you finish adding all your metrics, you need to serialize and flush them to standard output. You can do that automatically with the log_metrics decorator. This decorator also validates , serializes , and flushes all your metrics. During metrics validation, if no metrics are provided then a warning will be logged, but no exception will be raised. app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"ExampleService\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"BookingConfirmation\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"BookingConfirmation\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"ExampleService\" } Metric validation If metrics are provided, and any of the following criteria are not met, SchemaValidationError exception will be raised: Maximum of 9 dimensions Namespace is set, and no more than one Metric units must be supported by CloudWatch","title":"Flushing metrics"},{"location":"core/metrics/#raising-schemavalidationerror-on-empty-metrics","text":"If you want to ensure that at least one metric is emitted, you can pass raise_on_empty_metrics to the log_metrics decorator: app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.metrics import Metrics metrics = Metrics () @metrics . log_metrics ( raise_on_empty_metrics = True ) def lambda_handler ( evt , ctx ): ... Suppressing warning messages on empty metrics If you expect your function to execute without publishing metrics every time, you can suppress the warning with warnings.filterwarnings(\"ignore\", \"No metrics to publish*\") .","title":"Raising SchemaValidationError on empty metrics"},{"location":"core/metrics/#nesting-multiple-middlewares","text":"When using multiple middlewares, use log_metrics as your last decorator wrapping all subsequent ones to prevent early Metric validations when code hasn't been run yet. nested_middlewares.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Metrics , Tracer from aws_lambda_powertools.metrics import MetricUnit tracer = Tracer ( service = \"booking\" ) metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics @tracer . capture_lambda_handler def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 )","title":"Nesting multiple middlewares"},{"location":"core/metrics/#capturing-cold-start-metric","text":"You can optionally capture cold start metrics with log_metrics decorator via capture_cold_start_metric param. app.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Metrics metrics = Metrics ( service = \"ExampleService\" ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): ... If it's a cold start invocation, this feature will: Create a separate EMF blob solely containing a metric named ColdStart Add function_name and service dimensions This has the advantage of keeping cold start metric separate from your application metrics, where you might have unrelated dimensions. We do not emit 0 as a value for ColdStart metric for cost reasons. Let us know if you'd prefer a flag to override it","title":"Capturing cold start metric"},{"location":"core/metrics/#advanced","text":"","title":"Advanced"},{"location":"core/metrics/#adding-metadata","text":"You can add high-cardinality data as part of your Metrics log with add_metadata method. This is useful when you want to search highly contextual information along with your metrics in your logs. Info This will not be available during metrics visualization - Use dimensions for this purpose app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_metadata ( key = \"booking_id\" , value = \"booking_uuid\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"SuccessfulBooking\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"SuccessfulBooking\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"booking\" , \"booking_id\" : \"booking_uuid\" }","title":"Adding metadata"},{"location":"core/metrics/#single-metric-with-a-different-dimension","text":"CloudWatch EMF uses the same dimensions across all your metrics. Use single_metric if you have a metric that should have different dimensions. Info Generally, this would be an edge case since you pay for unique metric . Keep the following formula in mind: unique metric = (metric_name + dimension_name + dimension_value) single_metric.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import single_metric from aws_lambda_powertools.metrics import MetricUnit def lambda_handler ( evt , ctx ): with single_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 , namespace = \"ExampleApplication\" ) as metric : metric . add_dimension ( name = \"function_context\" , value = \"$LATEST\" ) ...","title":"Single metric with a different dimension"},{"location":"core/metrics/#flushing-metrics-manually","text":"If you prefer not to use log_metrics because you might want to encapsulate additional logic when doing so, you can manually flush and clear metrics as follows: Warning Metrics, dimensions and namespace validation still applies. manual_metric_serialization.py 1 2 3 4 5 6 7 8 9 10 11 import json from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 ) your_metrics_object = metrics . serialize_metric_set () metrics . clear_metrics () print ( json . dumps ( your_metrics_object ))","title":"Flushing metrics manually"},{"location":"core/metrics/#testing-your-code","text":"","title":"Testing your code"},{"location":"core/metrics/#environment-variables","text":"Use POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME env vars when unit testing your code to ensure metric namespace and dimension objects are created, and your code doesn't fail validation. 1 POWERTOOLS_SERVICE_NAME = \"Example\" POWERTOOLS_METRICS_NAMESPACE = \"Application\" python -m pytest If you prefer setting environment variable for specific tests, and are using Pytest, you can use monkeypatch fixture: pytest_env_var.py 1 2 3 4 5 6 def test_namespace_env_var ( monkeypatch ): # Set POWERTOOLS_METRICS_NAMESPACE before initializating Metrics monkeypatch . setenv ( \"POWERTOOLS_METRICS_NAMESPACE\" , namespace ) metrics = Metrics () ... Ignore this, if you are explicitly setting namespace/default dimension via namespace and service parameters: metrics = Metrics(namespace=ApplicationName, service=ServiceName)","title":"Environment variables"},{"location":"core/metrics/#clearing-metrics","text":"Metrics keep metrics in memory across multiple instances. If you need to test this behaviour, you can use the following Pytest fixture to ensure metrics are reset incl. cold start: pytest_metrics_reset_fixture.py 1 2 3 4 5 6 7 8 @pytest . fixture ( scope = \"function\" , autouse = True ) def reset_metric_set (): # Clear out every metric data prior to every test metrics = Metrics () metrics . clear_metrics () metrics_global . is_cold_start = True # ensure each test has cold start metrics . clear_default_dimensions () # remove persisted default dimensions, if any yield","title":"Clearing metrics"},{"location":"core/metrics/#functional-testing","text":"As metrics are logged to standard output, you can read standard output and assert whether metrics are present. Here's an example using pytest with capsys built-in fixture: Assert single EMF blob with pytest.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit import json def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN we utilize log_metrics to serialize # and flush all metrics at the end of a function execution @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) lambda_handler ({}, {}) log = capsys . readouterr () . out . strip () # remove any extra line metrics_output = json . loads ( log ) # deserialize JSON str # THEN we should have no exceptions # and a valid EMF object should be flushed correctly assert \"SuccessfulBooking\" in log # basic string assertion in JSON str assert \"SuccessfulBooking\" in metrics_output [ \"_aws\" ][ \"CloudWatchMetrics\" ][ 0 ][ \"Metrics\" ][ 0 ][ \"Name\" ] Assert multiple EMF blobs with pytest 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit from collections import namedtuple import json def capture_metrics_output_multiple_emf_objects ( capsys ): return [ json . loads ( line . strip ()) for line in capsys . readouterr () . out . split ( \" \\n \" ) if line ] def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN log_metrics is used with capture_cold_start_metric @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) # log_metrics uses function_name property from context to add as a dimension for cold start metric LambdaContext = namedtuple ( \"LambdaContext\" , \"function_name\" ) lambda_handler ({}, LambdaContext ( \"example_fn\" ) cold_start_blob , custom_metrics_blob = capture_metrics_output_multiple_emf_objects ( capsys ) # THEN ColdStart metric and function_name dimension should be logged # in a separate EMF blob than the application metrics assert cold_start_blob [ \"ColdStart\" ] == [ 1.0 ] assert cold_start_blob [ \"function_name\" ] == \"example_fn\" assert \"SuccessfulBooking\" in custom_metrics_blob # as per previous example For more elaborate assertions and comparisons, check out our functional testing for Metrics utility","title":"Functional testing"},{"location":"core/tracing/","text":"Powertools tracing is an opinionated thin wrapper for AWS X-Ray .NET SDK a provides functionality to reduce the overhead of performing common tracing tasks. Key Features Capture cold start as annotation, and responses as well as full exceptions as metadata Helper methods to improve the developer experience of creating new X-Ray subsegments. Better developer experience when developing with multiple threads. Auto patch supported modules by AWS X-Ray Initialization Before your use this utility, your AWS Lambda function must have permissions to send traces to AWS X-Ray. Example using AWS Serverless Application Model (SAM) template.yaml 1 2 3 4 5 6 7 8 9 10 11 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : ... Runtime : dotnetcore3.1 Tracing : Active Environment : Variables : POWERTOOLS_SERVICE_NAME : example The Powertools service name is used as the X-Ray namespace. This can be set using the environment variable POWERTOOLS_SERVICE_NAME Lambda handler \u00b6 To enable Powertools tracing to your function add the [Tracing] attribute to your FunctionHandler method or on any method will capture the method as a separate subsegment automatically. You can optionally choose to customize segment name that appears in traces. Tracing attribute 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class Function { [Tracing] public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { await BusinessLogic1 () . ConfigureAwait ( false ); await BusinessLogic2 () . ConfigureAwait ( false ); } [Tracing] private async Task BusinessLogic1 (){ } [Tracing] private async Task BusinessLogic2 (){ } } Custom Segment names 1 2 3 4 5 6 7 8 9 public class Function { [Tracing(SegmentName = \"YourCustomName\")] public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { ... } } By default, this attribute will automatically record method responses and exceptions. You can change the default behavior by setting the environment variables POWERTOOLS_TRACER_CAPTURE_RESPONSE and POWERTOOLS_TRACER_CAPTURE_ERROR as needed. Optionally, you can override behavior by different supported CaptureMode to record response, exception or both. Returning sensitive information from your Lambda handler or functions, where Tracing is used? You can disable attribute from capturing their responses and exception as tracing metadata with captureMode=DISABLED or globally by setting environment variables POWERTOOLS_TRACER_CAPTURE_RESPONSE and POWERTOOLS_TRACER_CAPTURE_ERROR to false Disable on attribute 1 2 3 4 5 6 7 8 9 public class Function { [Tracing(CaptureMode = TracingCaptureMode.Disabled)] public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { ... } } Disable Globally 1 2 3 4 5 6 7 8 9 10 11 12 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : ... Runtime : dotnetcore3.1 Tracing : Active Environment : Variables : POWERTOOLS_TRACER_CAPTURE_RESPONSE : false POWERTOOLS_TRACER_CAPTURE_ERROR : false Annotations & Metadata \u00b6 Annotations are key-values associated with traces and indexed by AWS X-Ray. You can use them to filter traces and to create Trace Groups to slice and dice your transactions. Metadata are key-values also associated with traces but not indexed by AWS X-Ray. You can use them to add additional context for an operation using any native object. Annotations You can add annotations using AddAnnotation() method from Tracing 1 2 3 4 5 6 7 8 9 10 11 using Amazon.Lambda.PowerTools.Tracing ; public class Function { [Tracing] public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { Tracing . AddAnnotation ( \"annotation\" , \"value\" ); } } Metadata You can add metadata using AddMetadata() method from Tracing 1 2 3 4 5 6 7 8 9 10 11 using Amazon.Lambda.PowerTools.Tracing ; public class Function { [Tracing] public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { Tracing . AddMetadata ( \"content\" , \"value\" ); } } Utilities \u00b6 Tracing modules comes with certain utility method when you don't want to use attribute for capturing a code block under a subsegment, or you are doing multithreaded programming. Refer examples below. Functional Api 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 using Amazon.Lambda.PowerTools.Tracing ; public class Function { public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { Tracing . WithSubsegment ( \"loggingResponse\" , ( subsegment ) => { // Some business logic }); Tracing . WithSubsegment ( \"localNamespace\" , \"loggingResponse\" , ( subsegment ) => { // Some business logic }); } } Multi Threaded Programming 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 using Amazon.Lambda.PowerTools.Tracing ; public class Function { public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { // Extract existing trace data var entity = Tracing . GetEntity (); var task = Task . Run (() => { Tracing . WithSubsegment ( \"InlineLog\" , entity , ( subsegment ) => { // Business logic in separate task }); }); } } Instrumenting SDK clients and HTTP calls \u00b6 User should make sure to instrument the SDK clients explicitly based on the function dependency. Refer details on how to instrument SDK client with Xray and outgoing http calls .","title":"Tracing"},{"location":"core/tracing/#lambda-handler","text":"To enable Powertools tracing to your function add the [Tracing] attribute to your FunctionHandler method or on any method will capture the method as a separate subsegment automatically. You can optionally choose to customize segment name that appears in traces. Tracing attribute 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class Function { [Tracing] public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { await BusinessLogic1 () . ConfigureAwait ( false ); await BusinessLogic2 () . ConfigureAwait ( false ); } [Tracing] private async Task BusinessLogic1 (){ } [Tracing] private async Task BusinessLogic2 (){ } } Custom Segment names 1 2 3 4 5 6 7 8 9 public class Function { [Tracing(SegmentName = \"YourCustomName\")] public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { ... } } By default, this attribute will automatically record method responses and exceptions. You can change the default behavior by setting the environment variables POWERTOOLS_TRACER_CAPTURE_RESPONSE and POWERTOOLS_TRACER_CAPTURE_ERROR as needed. Optionally, you can override behavior by different supported CaptureMode to record response, exception or both. Returning sensitive information from your Lambda handler or functions, where Tracing is used? You can disable attribute from capturing their responses and exception as tracing metadata with captureMode=DISABLED or globally by setting environment variables POWERTOOLS_TRACER_CAPTURE_RESPONSE and POWERTOOLS_TRACER_CAPTURE_ERROR to false Disable on attribute 1 2 3 4 5 6 7 8 9 public class Function { [Tracing(CaptureMode = TracingCaptureMode.Disabled)] public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { ... } } Disable Globally 1 2 3 4 5 6 7 8 9 10 11 12 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : ... Runtime : dotnetcore3.1 Tracing : Active Environment : Variables : POWERTOOLS_TRACER_CAPTURE_RESPONSE : false POWERTOOLS_TRACER_CAPTURE_ERROR : false","title":"Lambda handler"},{"location":"core/tracing/#annotations-metadata","text":"Annotations are key-values associated with traces and indexed by AWS X-Ray. You can use them to filter traces and to create Trace Groups to slice and dice your transactions. Metadata are key-values also associated with traces but not indexed by AWS X-Ray. You can use them to add additional context for an operation using any native object. Annotations You can add annotations using AddAnnotation() method from Tracing 1 2 3 4 5 6 7 8 9 10 11 using Amazon.Lambda.PowerTools.Tracing ; public class Function { [Tracing] public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { Tracing . AddAnnotation ( \"annotation\" , \"value\" ); } } Metadata You can add metadata using AddMetadata() method from Tracing 1 2 3 4 5 6 7 8 9 10 11 using Amazon.Lambda.PowerTools.Tracing ; public class Function { [Tracing] public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { Tracing . AddMetadata ( \"content\" , \"value\" ); } }","title":"Annotations &amp; Metadata"},{"location":"core/tracing/#utilities","text":"Tracing modules comes with certain utility method when you don't want to use attribute for capturing a code block under a subsegment, or you are doing multithreaded programming. Refer examples below. Functional Api 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 using Amazon.Lambda.PowerTools.Tracing ; public class Function { public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { Tracing . WithSubsegment ( \"loggingResponse\" , ( subsegment ) => { // Some business logic }); Tracing . WithSubsegment ( \"localNamespace\" , \"loggingResponse\" , ( subsegment ) => { // Some business logic }); } } Multi Threaded Programming 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 using Amazon.Lambda.PowerTools.Tracing ; public class Function { public async Task < APIGatewayProxyResponse > FunctionHandler ( APIGatewayProxyRequest apigProxyEvent , ILambdaContext context ) { // Extract existing trace data var entity = Tracing . GetEntity (); var task = Task . Run (() => { Tracing . WithSubsegment ( \"InlineLog\" , entity , ( subsegment ) => { // Business logic in separate task }); }); } }","title":"Utilities"},{"location":"core/tracing/#instrumenting-sdk-clients-and-http-calls","text":"User should make sure to instrument the SDK clients explicitly based on the function dependency. Refer details on how to instrument SDK client with Xray and outgoing http calls .","title":"Instrumenting SDK clients and HTTP calls"}]}